### 1.介绍

K-近邻算法(K Nearest Neighbor)又叫**KNN算法**，指如果一个样本在特征空间中的**k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别**，则该样本也属于这个类别。也就是对于新输入的实例，从数据集中找到于该实例最邻近的k个实例，那么这k个实例大多数属于某一个类，那么就把该实例放到该类中。

KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。

> **举个栗子：**若已经对一部分人标明皮肤白还是黑，当新加入一个人时，若要对其判定皮肤是黑还是白，那么就可以看其皮肤颜色与已判定黑或者白的人群皮肤颜色进行对比，与哪方更贴近则可以表明该人皮肤到底属于哪一类

### 2.K-近邻算法流程

* 计算已知类别数据集中的点与当前点之间的距离
* 按距离递增次序排序
* 选取与当前点距离最小的k个点
* 统计前k个点所在的类别出现的频率
* 返回前k个点出现频率最高的类别作为当前点的预测分类

> 计算距离一般使用欧氏距离公式，![欧式距离公式](image\欧式距离公式.svg)

### 3.Scikit-learn

Scikit-learn是python学习机器学习的工具，包括很多的机器学习算法。

优缺点

优点

简单，易理解，精度高，既可以用来做分类也可以用来做回归
可用于数值型数据和离散型数据
无数据输入假定
适合对稀有事件进行分类



缺点

简单好用，容易理解，精度高，理论成熟，既可以用来做分类也可以用来做回归
可用于数值型数据和离散型数据
无数据输入假定
适合对稀有事件进行分类







#### (1)安装

安装：`pip3 install scikit-learn`

> 注：安装scikit-learn需要Numpy, Scipy等库

#### (2)Scikit-learn包含的内容

- 分类、聚类、回归
- 特征工程
- 模型选择、调优

#### (3)K-近邻算法API---`sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,weights=’uniform’,algorithm=’auto’,leaf_size=30,p=2,metric=’minkowski’,metric_params=None,n_jobs=1) `

* n_neighbors=5：int,可选（默认= 5），k_neighbors查询默认使用的邻居数

* weights='uniform'：权重: str or callable(自定义类型), 可选参数(默认为 ‘uniform’)

  - ‘uniform’ : 统一的权重. 在每一个邻居区域里的点的权重都是一样的。


  - ‘distance’ : 权重点等于他们距离的倒数。使用此函数，更近的邻居对于所预测的点的影响更大。
  - [callable] : 一个用户自定义的方法，此方法接收一个距离的数组，然后返回一个相同形状并且包含权重的数组。

* algorithm（算法）: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, 可选参数（默认为 'auto'）,“auto”将尝试根据传递给fit方法的值来决定最合适的算法

* leaf_size：叶大小传递给BallTree或KDTree。这会影响构造和查询的速度，以及存储树所需的内存。最佳值取决于问题的性质。默认30。

* leaf_size=30,
  p=2,
  metric='minkowski',
  metric_params=None,
  n_jobs=None,
  )


- n_neighbors：

```python
```



